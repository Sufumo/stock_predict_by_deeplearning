"""
æ•°æ®é¢„å¤„ç†æµç¨‹è„šæœ¬
å®Œæˆæ•°æ®æ¸…æ´—ã€æ—¥æœŸèŒƒå›´è¿‡æ»¤ã€NaNå€¼å¤„ç†å’Œç»Ÿè®¡ä¿¡æ¯è¾“å‡º
"""
import os
import sys
from pathlib import Path
import json
import pandas as pd
import numpy as np
from typing import Dict

from components.data_preprocessor import DataPreprocessor, preprocess_data


def check_files_exist(data_path: str, relation_path: str) -> bool:
    """æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return False
    
    if not os.path.exists(relation_path):
        print(f"âŒ é”™è¯¯: å…³ç³»æ–‡ä»¶ä¸å­˜åœ¨: {relation_path}")
        return False
    
    return True


def validate_data_format(data_path: str) -> bool:
    """éªŒè¯æ•°æ®æ ¼å¼"""
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, dict):
            print("âŒ é”™è¯¯: æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”è¯¥æ˜¯å­—å…¸æ ¼å¼")
            return False
        
        # æ£€æŸ¥è‡³å°‘ä¸€ä¸ªè¡Œä¸šçš„æ•°æ®æ ¼å¼
        sample_industry = list(data.keys())[0] if data else None
        if sample_industry and data[sample_industry]:
            sample_kline = data[sample_industry][0]
            if not isinstance(sample_kline, list) or len(sample_kline) < 7:
                print("âŒ é”™è¯¯: Kçº¿æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œæ¯ä¸ªKçº¿åº”è¯¥æ˜¯åŒ…å«è‡³å°‘7ä¸ªå…ƒç´ çš„åˆ—è¡¨")
                print(f"   ç¤ºä¾‹: [æ—¥æœŸ, å¼€ç›˜, æ”¶ç›˜, æœ€é«˜, æœ€ä½, æˆäº¤é‡, æˆäº¤é¢, ...]")
                return False
        
        return True
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSONè§£æå¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ é”™è¯¯: éªŒè¯æ•°æ®æ ¼å¼æ—¶å‡ºé”™: {e}")
        return False


def print_preprocessing_summary(preprocessor: DataPreprocessor, 
                                cleaned_data: Dict,
                                valid_relation_df: pd.DataFrame):
    """æ‰“å°é¢„å¤„ç†æ‘˜è¦"""
    print("\n" + "=" * 80)
    print("é¢„å¤„ç†æ‘˜è¦")
    print("=" * 80)
    
    stats = preprocessor.stats
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»è¡Œä¸šæ•°: {stats['total_industries']}")
    print(f"  æœ‰æ•ˆè¡Œä¸šæ•°: {stats['valid_industries']}")
    print(f"  ç§»é™¤è¡Œä¸šæ•°: {len(stats['removed_industries'])}")
    print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples_after']:,}")
    print(f"  æ—¥æœŸèŒƒå›´: {preprocessor.start_date.strftime('%Y-%m-%d')} åˆ° {preprocessor.end_date.strftime('%Y-%m-%d')}")
    
    if stats['removed_industries']:
        print(f"\nâš ï¸  ç§»é™¤çš„è¡Œä¸š ({len(stats['removed_industries'])}ä¸ª):")
        for removed in stats['removed_industries'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"    - {removed['industry']}: {removed['reason']}")
        if len(stats['removed_industries']) > 10:
            print(f"    ... è¿˜æœ‰ {len(stats['removed_industries']) - 10} ä¸ªè¡Œä¸šè¢«ç§»é™¤")
    
    # æ ·æœ¬æ•°ç»Ÿè®¡
    if cleaned_data:
        samples_list = [len(arr) for arr in cleaned_data.values()]
        print(f"\nğŸ“ˆ æ ·æœ¬æ•°åˆ†å¸ƒ:")
        print(f"  æœ€å°å€¼: {min(samples_list):,}")
        print(f"  æœ€å¤§å€¼: {max(samples_list):,}")
        print(f"  å¹³å‡å€¼: {np.mean(samples_list):.0f}")
        print(f"  ä¸­ä½æ•°: {np.median(samples_list):.0f}")
        print(f"  æ ‡å‡†å·®: {np.std(samples_list):.0f}")
    
    # NaNç»Ÿè®¡
    total_nan_before = sum(s.get('before', 0) for s in stats['nan_counts'].values())
    total_nan_after = sum(s.get('after', 0) for s in stats['nan_counts'].values())
    if total_nan_before > 0:
        print(f"\nğŸ”§ NaNå€¼å¤„ç†:")
        print(f"  å¤„ç†å‰NaNæ€»æ•°: {total_nan_before:,}")
        print(f"  å¤„ç†åNaNæ€»æ•°: {total_nan_after:,}")
        if total_nan_before > 0:
            print(f"  å¤„ç†ç‡: {(1 - total_nan_after / total_nan_before) * 100:.2f}%")
    
    # æ—¥æœŸè¿‡æ»¤ç»Ÿè®¡
    total_date_filtered = sum(stats['date_filtered_counts'].values())
    if total_date_filtered > 0:
        print(f"\nğŸ“… æ—¥æœŸè¿‡æ»¤:")
        print(f"  è¿‡æ»¤æ‰çš„æ ·æœ¬æ€»æ•°: {total_date_filtered:,}")


def save_sample_counts(sample_counts: Dict[str, int], output_path: str):
    """ä¿å­˜æ¯ä¸ªè¡Œä¸šçš„æ ·æœ¬æ•°åˆ°æ–‡ä»¶"""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # è½¬æ¢ä¸ºDataFrameä¾¿äºæŸ¥çœ‹
    df = pd.DataFrame([
        {'industry': industry, 'samples': count}
        for industry, count in sorted(sample_counts.items(), key=lambda x: x[1], reverse=True)
    ])
    
    # ä¿å­˜ä¸ºCSV
    csv_path = output_path.with_suffix('.csv')
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ æ ·æœ¬æ•°ç»Ÿè®¡å·²ä¿å­˜åˆ°: {csv_path}")
    
    # ä¿å­˜ä¸ºJSON
    json_path = output_path.with_suffix('.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(sample_counts, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ æ ·æœ¬æ•°ç»Ÿè®¡å·²ä¿å­˜åˆ°: {json_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("æ•°æ®é¢„å¤„ç†æµç¨‹")
    print("=" * 80)
    
    # ========== é…ç½®å‚æ•° ==========
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    data_path = "./data/industry_kline_data.json"
    relation_path = "./data/industry_relation.csv"
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_data_path = "./data/industry_kline_data_cleaned.json"
    output_relation_path = "./data/industry_relation_cleaned.csv"
    report_path = "./data/cleaning_report.json"
    sample_counts_path = "./data/industry_sample_counts"
    
    # é¢„å¤„ç†å‚æ•°
    start_date = '2021-12-01'  # å¼€å§‹æ—¥æœŸ
    end_date = '2025-11-17'    # ç»“æŸæ—¥æœŸ
    nan_strategy = 'forward_fill'  # NaNå¤„ç†ç­–ç•¥ï¼šforward_fillï¼ˆå‰å‘å¡«å……ï¼‰
    min_valid_samples = 100  # æ¯ä¸ªè¡Œä¸šæœ€å°‘æœ‰æ•ˆæ ·æœ¬æ•°
    
    # æ˜¾ç¤ºå‚æ•°
    top_n_industries = 20  # æ˜¾ç¤ºå‰Nä¸ªè¡Œä¸šçš„è¯¦ç»†ç»Ÿè®¡
    
    # ========== æ–‡ä»¶æ£€æŸ¥ ==========
    print("\nğŸ“ æ£€æŸ¥è¾“å…¥æ–‡ä»¶...")
    if not check_files_exist(data_path, relation_path):
        sys.exit(1)
    
    print("âœ… è¾“å…¥æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    # ========== æ•°æ®æ ¼å¼éªŒè¯ ==========
    print("\nğŸ” éªŒè¯æ•°æ®æ ¼å¼...")
    if not validate_data_format(data_path):
        sys.exit(1)
    
    print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
    
    # ========== æ‰§è¡Œé¢„å¤„ç† ==========
    print("\nğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    print(f"   æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
    print(f"   NaNå¤„ç†ç­–ç•¥: {nan_strategy}")
    print(f"   æœ€å°‘æ ·æœ¬æ•°: {min_valid_samples}")
    print("-" * 80)
    
    try:
        cleaned_data, valid_relation_df, preprocessor = preprocess_data(
            data_path=data_path,
            relation_path=relation_path,
            output_data_path=output_data_path,
            output_relation_path=output_relation_path,
            start_date=start_date,
            end_date=end_date,
            nan_strategy=nan_strategy,
            min_valid_samples=min_valid_samples,
            save_report=True,
            report_path=report_path,
            verbose=True
        )
        
        print("\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ é¢„å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # ========== æ‰“å°ç»Ÿè®¡ä¿¡æ¯ ==========
    print("\nğŸ“Š æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯...")
    preprocessor.print_industry_stats(sort_by='samples', top_n=top_n_industries)
    
    # ========== æ‰“å°é¢„å¤„ç†æ‘˜è¦ ==========
    print_preprocessing_summary(preprocessor, cleaned_data, valid_relation_df)
    
    # ========== è·å–å¹¶ä¿å­˜æ ·æœ¬æ•°ç»Ÿè®¡ ==========
    print("\nğŸ“ˆ è·å–å„è¡Œä¸šæ ·æœ¬æ•°...")
    sample_counts = preprocessor.get_industry_sample_counts()
    
    # æ‰“å°æ ·æœ¬æ•°ç»Ÿè®¡ï¼ˆå‰20ä¸ªï¼‰
    print("\næ ·æœ¬æ•°æœ€å¤šçš„å‰20ä¸ªè¡Œä¸š:")
    sorted_counts = sorted(sample_counts.items(), key=lambda x: x[1], reverse=True)
    for i, (industry, count) in enumerate(sorted_counts[:20], 1):
        print(f"  {i:2d}. {industry:<20} {count:>8,} æ ·æœ¬")
    
    # ä¿å­˜æ ·æœ¬æ•°ç»Ÿè®¡
    save_sample_counts(sample_counts, sample_counts_path)
    
    # ========== éªŒè¯è¾“å‡ºæ–‡ä»¶ ==========
    print("\nğŸ” éªŒè¯è¾“å‡ºæ–‡ä»¶...")
    if os.path.exists(output_data_path):
        file_size = os.path.getsize(output_data_path) / (1024 * 1024)  # MB
        print(f"âœ… æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶å·²ä¿å­˜: {output_data_path} ({file_size:.2f} MB)")
    else:
        print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {output_data_path}")
    
    if os.path.exists(output_relation_path):
        print(f"âœ… æ¸…æ´—åçš„å…³ç³»æ–‡ä»¶å·²ä¿å­˜: {output_relation_path}")
    else:
        print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºå…³ç³»æ–‡ä»¶ä¸å­˜åœ¨: {output_relation_path}")
    
    if os.path.exists(report_path):
        print(f"âœ… æ¸…æ´—æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # ========== å®Œæˆ ==========
    print("\n" + "=" * 80)
    print("âœ… æ•°æ®é¢„å¤„ç†æµç¨‹å®Œæˆï¼")
    print("=" * 80)
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  ğŸ“„ æ¸…æ´—åçš„æ•°æ®: {output_data_path}")
    print(f"  ğŸ“„ æ¸…æ´—åçš„å…³ç³»: {output_relation_path}")
    print(f"  ğŸ“„ æ¸…æ´—æŠ¥å‘Š: {report_path}")
    print(f"  ğŸ“„ æ ·æœ¬æ•°ç»Ÿè®¡: {sample_counts_path}.csv / {sample_counts_path}.json")
    print("\nå¯ä»¥å¼€å§‹ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®è¿›è¡Œè®­ç»ƒäº†ï¼")


if __name__ == "__main__":
    main()

