"""
è®­ç»ƒå™¨ç»„ä»¶
è´Ÿè´£æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œè¯„ä¼°
æ”¯æŒKæŠ˜éªŒè¯ã€é‡‘èæŒ‡æ ‡ã€å­¦ä¹ ç‡è°ƒåº¦ç­‰
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from typing import Dict, Optional, Tuple, List
import numpy as np
from tqdm import tqdm
import os
import shutil
from pathlib import Path

try:
    from .metrics import FinancialMetricsCalculator
    from .validator import TimeSeriesKFold
    from .monitor import NaNDetector, GradientMonitor
except ImportError:
    # å¦‚æœæ˜¯ç›´æ¥è¿è¡Œè¯¥æ–‡ä»¶
    from metrics import FinancialMetricsCalculator
    from validator import TimeSeriesKFold
    from monitor import NaNDetector, GradientMonitor


class Trainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, device: Optional[torch.device] = None,
                 learning_rate: float = 1e-4, weight_decay: float = 1e-5,
                 use_scheduler: bool = False, scheduler_params: Optional[Dict] = None,
                 compute_financial_metrics: bool = True, max_grad_norm: Optional[float] = None):
        """
        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            device: è®­ç»ƒè®¾å¤‡ï¼ˆCPU/GPUï¼‰
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
            use_scheduler: æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler_params: è°ƒåº¦å™¨å‚æ•°
            compute_financial_metrics: æ˜¯å¦è®¡ç®—é‡‘èæŒ‡æ ‡
            max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼
        """
        self.model = model
        self.device = device if device is not None else torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu'
        )
        self.model.to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.use_scheduler = use_scheduler
        self.scheduler = None
        if use_scheduler:
            if scheduler_params is None:
                scheduler_params = {'mode': 'min', 'factor': 0.5, 'patience': 5}
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, **scheduler_params
            )

        # æ¢¯åº¦è£å‰ª
        self.max_grad_norm = max_grad_norm

        # æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰
        self.criterion = nn.CrossEntropyLoss()

        # é‡‘èæŒ‡æ ‡è®¡ç®—å™¨
        self.compute_financial_metrics = compute_financial_metrics
        if compute_financial_metrics:
            self.metrics_calculator = FinancialMetricsCalculator()

        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [],
            'accuracy': []
        }
        self.val_history = {
            'loss': [],
            'accuracy': []
        }

        # é‡‘èæŒ‡æ ‡å†å²
        if compute_financial_metrics:
            self.val_history['IC'] = []
            self.val_history['RankIC'] = []
            self.val_history['long_short_return'] = []

        # â­ NaN/Infæ£€æµ‹å™¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.nan_detector = NaNDetector(model, check_frequency=50)
        self.enable_nan_detection = False  # é»˜è®¤å…³é—­ï¼Œå¯åœ¨è®­ç»ƒæ—¶å¼€å¯

        # â­ æ¢¯åº¦ç›‘æ§å™¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.gradient_monitor = GradientMonitor(model)
        self.enable_gradient_monitor = False  # é»˜è®¤å…³é—­

    def enable_debugging(self, enable_nan_detection: bool = True, enable_gradient_monitor: bool = False):
        """
        å¯ç”¨è°ƒè¯•æ¨¡å¼

        Args:
            enable_nan_detection: å¯ç”¨NaN/Infæ£€æµ‹
            enable_gradient_monitor: å¯ç”¨æ¢¯åº¦ç›‘æ§
        """
        self.enable_nan_detection = enable_nan_detection
        self.enable_gradient_monitor = enable_gradient_monitor

        if enable_nan_detection:
            self.nan_detector.enable()
            print("âœ“ NaN/Inf detection enabled")

        if enable_gradient_monitor:
            self.gradient_monitor.register_hooks()
            print("âœ“ Gradient monitoring enabled")

    def disable_debugging(self):
        """ç¦ç”¨è°ƒè¯•æ¨¡å¼"""
        self.enable_nan_detection = False
        self.enable_gradient_monitor = False
        self.nan_detector.disable()
        self.gradient_monitor.remove_hooks()
        print("âœ“ Debugging disabled")

    def print_monitoring_report(self):
        """æ‰“å°ç›‘æ§æŠ¥å‘Š"""
        if self.enable_gradient_monitor:
            self.gradient_monitor.print_summary(top_k=10)
        if self.enable_nan_detection:
            self.nan_detector.print_report()

    def train_epoch(self, dataloader: DataLoader, adj_matrix: torch.Tensor) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            adj_matrix: é‚»æ¥çŸ©é˜µ
            
        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        adj_matrix = adj_matrix.to(self.device)
        
        pbar = tqdm(dataloader, desc='Training')
        for batch in pbar:
            # å‡†å¤‡æ•°æ®
            sequences = batch['sequence'].to(self.device)  # [batch_size, max_seq_len, features]
            targets = batch['target'].to(self.device)  # [batch_size]
            masks = batch['mask'].to(self.device)  # [batch_size, max_seq_len]
            industry_indices = batch['industry_idx'].to(self.device)  # [batch_size]
            
            batch_size, max_seq_len, features = sequences.shape
            
            # æå–ä¸åŒæ—¶é—´çª—å£çš„æ•°æ®
            # å‡è®¾max_seq_len=80ï¼Œåˆ™ï¼š
            # - x_80: å…¨éƒ¨80ä¸ªæ—¶é—´æ­¥
            # - x_40: æœ€å40ä¸ªæ—¶é—´æ­¥
            # - x_20: æœ€å20ä¸ªæ—¶é—´æ­¥
            x_80 = sequences  # [batch_size, 80, features]
            x_40 = sequences[:, -40:, :]  # [batch_size, 40, features]
            x_20 = sequences[:, -20:, :]  # [batch_size, 20, features]
            
            # å¯¹åº”çš„æ©ç 
            mask_80 = masks  # [batch_size, 80]
            mask_40 = masks[:, -40:]  # [batch_size, 40]
            mask_20 = masks[:, -20:]  # [batch_size, 20]
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predictions, _ = self.model(
                x_20, x_40, x_80,
                mask_20, mask_40, mask_80,
                adj_matrix, industry_indices
            )
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(predictions, targets)

            # â­ NaN/Infæ£€æµ‹ï¼ˆåœ¨åå‘ä¼ æ’­å‰ï¼‰
            if self.enable_nan_detection:
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"\nâŒ NaN/Inf detected in loss!")
                    print(f"   Loss value: {loss.item()}")
                    print(f"   Predictions stats: mean={predictions.mean().item():.4f}, "
                          f"std={predictions.std().item():.4f}, "
                          f"min={predictions.min().item():.4f}, "
                          f"max={predictions.max().item():.4f}")
                    print(f"   Targets: {targets[:10].cpu().numpy()}")
                    self.nan_detector.print_report()
                    raise ValueError("Training collapsed due to NaN/Inf loss!")

            # åå‘ä¼ æ’­
            loss.backward()

            # â­ NaNæ£€æµ‹ï¼ˆæ¢¯åº¦ï¼‰
            if self.enable_nan_detection:
                if not self.nan_detector.step(loss):
                    self.nan_detector.print_report()
                    raise ValueError("Training collapsed due to NaN/Inf in gradients!")

            # æ¢¯åº¦è£å‰ª
            if self.max_grad_norm is not None:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)

            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            pred_classes = predictions.argmax(dim=1)
            correct += (pred_classes == targets).sum().item()
            total += targets.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100 * correct / total:.2f}%'
            })
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100 * correct / total if total > 0 else 0.0
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy
        }

    def train_epoch_cross_sectional(self, dataloader: DataLoader, adj_matrix: torch.Tensor,
                                    epoch: int = 0) -> Dict[str, float]:
        """
        æ¨ªæˆªé¢å±€éƒ¨è®­ç»ƒæ¨¡å¼çš„è®­ç»ƒepoch

        ç‰¹ç‚¹ï¼š
        - æ—¶é—´æ­¥è¿½è¸ª
        - æ”¯æŒnode_mask
        - è®°å½•é—¨æ§å€¼ç»Ÿè®¡

        Args:
            dataloader: CrossSectionalLocalDatasetçš„DataLoader
            adj_matrix: å®Œæ•´86èŠ‚ç‚¹é‚»æ¥çŸ©é˜µ
            epoch: å½“å‰epochç¼–å·

        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        adj_matrix = adj_matrix.to(self.device)

        # æ—¶é—´æ­¥è¿½è¸ª
        current_time_step = -1
        time_step_losses = []
        time_step_accs = []
        
        # â­ å­˜å‚¨æ¯ä¸ªæ—¶é—´æ­¥çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºæœ€åç»Ÿä¸€è¾“å‡ºï¼‰
        time_step_stats = {}  # {time_step: {'losses': [...], 'accs': [...]}}

        # é—¨æ§å€¼ç»Ÿè®¡
        all_gate_values = []

        pbar = tqdm(dataloader, desc=f'Training Epoch {epoch+1}')
        for batch_idx, batch in enumerate(pbar):
            # â­ å¤„ç†å¯å˜å¤§å°çš„batchï¼ˆä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°ï¼‰
            # batch['sequence'] ç­‰æ˜¯åˆ—è¡¨ï¼Œéœ€è¦é€ä¸ªå¤„ç†æˆ–åˆå¹¶
            sequences_list = batch['sequence']  # List of [num_active_i, max_seq_len, features]
            targets_list = batch['target']  # List of [num_active_i]
            masks_list = batch['mask']  # List of [num_active_i, max_seq_len]
            industry_indices_list = batch['industry_idx']  # List of [num_active_i]
            node_mask = batch['node_mask'].to(self.device)  # [batch_size, 86]
            time_indices = batch['time_index']  # [batch_size]
            
            # â­ åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„åºåˆ—ï¼ˆå› ä¸ºæ¯ä¸ªæ ·æœ¬çš„num_activeå¯èƒ½ä¸åŒï¼‰
            # å°†æ‰€æœ‰æ ·æœ¬çš„åºåˆ—ã€ç›®æ ‡ç­‰åˆå¹¶æˆä¸€ä¸ªå¤§çš„batch
            all_sequences = []
            all_targets = []
            all_masks = []
            all_industry_indices = []
            all_node_masks = []
            
            for i in range(len(sequences_list)):
                all_sequences.append(sequences_list[i].to(self.device))
                all_targets.append(targets_list[i].to(self.device))
                all_masks.append(masks_list[i].to(self.device))
                all_industry_indices.append(industry_indices_list[i].to(self.device))
                all_node_masks.append(node_mask[i])  # [86]
            
            # åˆå¹¶æ‰€æœ‰åºåˆ—
            sequences = torch.cat(all_sequences, dim=0)  # [total_active, max_seq_len, features]
            targets = torch.cat(all_targets, dim=0)  # [total_active]
            masks = torch.cat(all_masks, dim=0)  # [total_active, max_seq_len]
            industry_indices = torch.cat(all_industry_indices, dim=0)  # [total_active]
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬çš„time_indexï¼ˆé€šå¸¸batchä¸­æ‰€æœ‰æ ·æœ¬æ¥è‡ªåŒä¸€æ—¶é—´æ­¥ï¼‰
            time_idx = time_indices[0].item() if len(time_indices) > 0 else -1

            # æ£€æŸ¥æ˜¯å¦è¿›å…¥æ–°çš„æ—¶é—´æ­¥
            if time_idx != current_time_step:
                if current_time_step >= 0:
                    # â­ å­˜å‚¨ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸æ‰“å°ï¼‰
                    if len(time_step_losses) > 0:
                        time_step_stats[current_time_step] = {
                            'losses': time_step_losses.copy(),
                            'accs': time_step_accs.copy()
                        }

                current_time_step = time_idx
                time_step_losses = []
                time_step_accs = []

            num_active, max_seq_len, features = sequences.shape

            # æå–ä¸åŒæ—¶é—´çª—å£çš„æ•°æ®
            x_80 = sequences  # [num_active, 80, features]
            x_40 = sequences[:, -40:, :]  # [num_active, 40, features]
            x_20 = sequences[:, -20:, :]  # [num_active, 20, features]

            # å¯¹åº”çš„æ©ç 
            mask_80 = masks
            mask_40 = masks[:, -40:]
            mask_20 = masks[:, -20:]

            # å‰å‘ä¼ æ’­ï¼ˆæ¨ªæˆªé¢æ¨¡å¼ï¼‰
            # â­ æ³¨æ„ï¼šç”±äºåˆå¹¶äº†å¤šä¸ªæ ·æœ¬ï¼Œnode_maskéœ€è¦ç‰¹æ®Šå¤„ç†
            # è¿™é‡Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬çš„node_maskï¼ˆé€šå¸¸batchä¸­æ‰€æœ‰æ ·æœ¬æ¥è‡ªåŒä¸€æ—¶é—´æ­¥ï¼‰
            # å¦‚æœéœ€è¦æ›´ç²¾ç¡®çš„å¤„ç†ï¼Œå¯ä»¥åˆ†åˆ«å¤„ç†æ¯ä¸ªæ ·æœ¬
            batch_node_mask = all_node_masks[0] if len(all_node_masks) > 0 else node_mask[0]
            
            self.optimizer.zero_grad()
            predictions, _, gates = self.model(
                x_20, x_40, x_80,
                mask_20, mask_40, mask_80,
                adj_matrix, industry_indices,
                node_mask=batch_node_mask  # â­ ä¼ é€’node_mask
            )

            # è®¡ç®—æŸå¤±
            loss = self.criterion(predictions, targets)

            # NaN/Infæ£€æµ‹
            if self.enable_nan_detection:
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"\nâŒ NaN/Inf detected in loss!")
                    print(f"   Time step: {time_idx}")
                    print(f"   Batch: {batch_idx}")
                    print(f"   Loss value: {loss.item()}")
                    self.nan_detector.print_report()
                    raise ValueError("Training collapsed!")

            # åå‘ä¼ æ’­
            loss.backward()

            # NaNæ£€æµ‹ï¼ˆæ¢¯åº¦ï¼‰
            if self.enable_nan_detection:
                if not self.nan_detector.step(loss):
                    self.nan_detector.print_report()
                    raise ValueError("Training collapsed!")

            # æ¢¯åº¦è£å‰ª
            if self.max_grad_norm is not None:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)

            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item() * num_active
            _, predicted = torch.max(predictions.data, 1)
            correct += (predicted == targets).sum().item()
            total += num_active

            # è®°å½•æ—¶é—´æ­¥ç»Ÿè®¡
            time_step_losses.append(loss.item())
            batch_acc = 100.0 * (predicted == targets).float().mean().item()
            time_step_accs.append(batch_acc)

            # æ”¶é›†é—¨æ§å€¼
            if gates is not None:
                all_gate_values.append(gates.detach().cpu())

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{batch_acc:.1f}%',
                'time_step': time_idx
            })

        # â­ å­˜å‚¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç»Ÿè®¡ä¿¡æ¯
        if len(time_step_losses) > 0:
            time_step_stats[current_time_step] = {
                'losses': time_step_losses.copy(),
                'accs': time_step_accs.copy()
            }

        avg_loss = total_loss / total if total > 0 else 0.0
        accuracy = 100 * correct / total if total > 0 else 0.0
        
        # â­ ç»Ÿä¸€è¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥çš„ç»Ÿè®¡ä¿¡æ¯
        if len(time_step_stats) > 0:
            print(f"\n{'='*60}")
            print(f"Time Step Statistics (Epoch {epoch+1}):")
            print(f"{'='*60}")
            
            # æŒ‰æ—¶é—´æ­¥æ’åº
            sorted_time_steps = sorted(time_step_stats.keys())
            
            # æ¯10ä¸ªæ—¶é—´æ­¥æ‰“å°ä¸€æ¬¡æ‘˜è¦ï¼Œæˆ–å…¨éƒ¨æ‰“å°ï¼ˆå¦‚æœæ—¶é—´æ­¥æ•°è¾ƒå°‘ï¼‰
            if len(sorted_time_steps) <= 50:
                # æ‰“å°æ‰€æœ‰æ—¶é—´æ­¥
                for ts in sorted_time_steps:
                    stats = time_step_stats[ts]
                    avg_ts_loss = np.mean(stats['losses'])
                    avg_ts_acc = np.mean(stats['accs'])
                    print(f"  Time step {ts:4d}: Loss={avg_ts_loss:.4f}, Acc={avg_ts_acc:.2f}%")
            else:
                # æ‰“å°æ‘˜è¦ï¼šæ¯10ä¸ªæ—¶é—´æ­¥æ‰“å°ä¸€æ¬¡
                print(f"  Total time steps: {len(sorted_time_steps)}")
                print(f"  Showing summary (every 10th time step):")
                for i, ts in enumerate(sorted_time_steps):
                    if i % 10 == 0 or i == len(sorted_time_steps) - 1:
                        stats = time_step_stats[ts]
                        avg_ts_loss = np.mean(stats['losses'])
                        avg_ts_acc = np.mean(stats['accs'])
                        print(f"  Time step {ts:4d}: Loss={avg_ts_loss:.4f}, Acc={avg_ts_acc:.2f}%")
            
            # æ‰“å°æ€»ä½“ç»Ÿè®¡
            all_losses = []
            all_accs = []
            for stats in time_step_stats.values():
                all_losses.extend(stats['losses'])
                all_accs.extend(stats['accs'])
            
            if len(all_losses) > 0:
                print(f"\n  Overall Statistics:")
                print(f"    Mean Loss: {np.mean(all_losses):.4f} Â± {np.std(all_losses):.4f}")
                print(f"    Mean Acc:  {np.mean(all_accs):.2f}% Â± {np.std(all_accs):.2f}%")
                print(f"    Min Acc:   {np.min(all_accs):.2f}%")
                print(f"    Max Acc:   {np.max(all_accs):.2f}%")
            
            print(f"{'='*60}")

        # è®¡ç®—é—¨æ§å€¼ç»Ÿè®¡
        gate_stats = {}
        if len(all_gate_values) > 0:
            all_gates_tensor = torch.cat(all_gate_values, dim=0)  # [total_active_nodes, 1]
            gate_stats = {
                'gate_mean': all_gates_tensor.mean().item(),
                'gate_std': all_gates_tensor.std().item(),
                'gate_min': all_gates_tensor.min().item(),
                'gate_max': all_gates_tensor.max().item(),
                'favor_time_ratio': (all_gates_tensor > 0.5).float().mean().item(),
                'favor_embedding_ratio': (all_gates_tensor <= 0.5).float().mean().item()
            }

        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            **gate_stats
        }

    def validate(self, dataloader: DataLoader, adj_matrix: torch.Tensor,
                compute_metrics: bool = True) -> Dict[str, float]:
        """
        éªŒè¯æ¨¡å‹

        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            adj_matrix: é‚»æ¥çŸ©é˜µ
            compute_metrics: æ˜¯å¦è®¡ç®—é‡‘èæŒ‡æ ‡

        Returns:
            éªŒè¯æŒ‡æ ‡å­—å…¸
        """
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºé‡‘èæŒ‡æ ‡è®¡ç®—
        all_predictions_prob = []
        all_targets = []
        all_returns = []

        adj_matrix = adj_matrix.to(self.device)

        with torch.no_grad():
            pbar = tqdm(dataloader, desc='Validating')
            for batch in pbar:
                # å‡†å¤‡æ•°æ®
                sequences = batch['sequence'].to(self.device)
                targets = batch['target'].to(self.device)
                masks = batch['mask'].to(self.device)
                industry_indices = batch['industry_idx'].to(self.device)
                
                batch_size, max_seq_len, features = sequences.shape
                
                # æå–ä¸åŒæ—¶é—´çª—å£çš„æ•°æ®
                x_80 = sequences
                x_40 = sequences[:, -40:, :]
                x_20 = sequences[:, -20:, :]
                
                mask_80 = masks
                mask_40 = masks[:, -40:]
                mask_20 = masks[:, -20:]
                
                # å‰å‘ä¼ æ’­
                predictions, _ = self.model(
                    x_20, x_40, x_80,
                    mask_20, mask_40, mask_80,
                    adj_matrix, industry_indices
                )
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(predictions, targets)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                pred_classes = predictions.argmax(dim=1)
                correct += (pred_classes == targets).sum().item()
                total += targets.size(0)

                # æ”¶é›†æ•°æ®ç”¨äºé‡‘èæŒ‡æ ‡
                if compute_metrics and self.compute_financial_metrics:
                    all_predictions_prob.append(predictions.cpu().numpy())
                    all_targets.append(targets.cpu().numpy())
                    # å°è¯•è·å–çœŸå®æ”¶ç›Šç‡
                    if 'return' in batch:
                        all_returns.append(batch['return'].cpu().numpy())

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100 * correct / total:.2f}%'
                })

        avg_loss = total_loss / len(dataloader)
        accuracy = 100 * correct / total if total > 0 else 0.0

        results = {
            'loss': avg_loss,
            'accuracy': accuracy
        }

        # è®¡ç®—é‡‘èæŒ‡æ ‡
        if compute_metrics and self.compute_financial_metrics and len(all_predictions_prob) > 0:
            all_predictions_prob = np.concatenate(all_predictions_prob, axis=0)
            all_targets = np.concatenate(all_targets, axis=0)

            # ä½¿ç”¨é¢„æµ‹æ¦‚ç‡çš„æœ€é«˜ç±»åˆ«ä½œä¸ºåˆ†æ•°
            pred_scores = np.max(all_predictions_prob, axis=1)

            # å¦‚æœæœ‰çœŸå®æ”¶ç›Šç‡æ•°æ®,è®¡ç®—ICç­‰æŒ‡æ ‡
            if len(all_returns) > 0:
                all_returns = np.concatenate(all_returns, axis=0)
                financial_metrics = self.metrics_calculator.compute_all_metrics(
                    pred_scores, all_returns
                )
                results.update(financial_metrics)
            else:
                # å¦‚æœæ²¡æœ‰æ”¶ç›Šç‡æ•°æ®,ä½¿ç”¨ç›®æ ‡ç±»åˆ«ä½œä¸ºæ›¿ä»£
                # å°†ç±»åˆ«è½¬æ¢ä¸ºè¿ç»­å€¼(-2, -1, 0, 1, 2)ç”¨äºç›¸å…³æ€§è®¡ç®—
                pseudo_returns = all_targets - 2.0  # å‡è®¾5ç±»:0,1,2,3,4 -> -2,-1,0,1,2
                financial_metrics = self.metrics_calculator.compute_all_metrics(
                    pred_scores, pseudo_returns
                )
                results.update(financial_metrics)

        return results
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader,
              adj_matrix: torch.Tensor, num_epochs: int = 50,
              save_path: Optional[str] = None,
              use_cross_sectional: bool = False) -> Dict[str, list]:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            adj_matrix: é‚»æ¥çŸ©é˜µ
            num_epochs: è®­ç»ƒè½®æ•°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            use_cross_sectional: æ˜¯å¦ä½¿ç”¨æ¨ªæˆªé¢è®­ç»ƒæ¨¡å¼

        Returns:
            è®­ç»ƒå†å²å­—å…¸
        """
        best_val_acc = 0.0

        # â­ å¦‚æœä½¿ç”¨èŠ‚ç‚¹çº§é—¨æ§ï¼Œæ·»åŠ é—¨æ§ç»Ÿè®¡è®°å½•
        if use_cross_sectional:
            self.train_history['gate_mean'] = []
            self.train_history['gate_std'] = []
            self.train_history['favor_time_ratio'] = []

        for epoch in range(num_epochs):
            print(f'\nEpoch {epoch + 1}/{num_epochs}')
            print('-' * 50)

            # â­ æ ¹æ®æ¨¡å¼é€‰æ‹©è®­ç»ƒæ–¹æ³•
            if use_cross_sectional:
                train_metrics = self.train_epoch_cross_sectional(train_loader, adj_matrix, epoch)
            else:
                train_metrics = self.train_epoch(train_loader, adj_matrix)

            self.train_history['loss'].append(train_metrics['loss'])
            self.train_history['accuracy'].append(train_metrics['accuracy'])

            # è®°å½•é—¨æ§ç»Ÿè®¡
            if use_cross_sectional and 'gate_mean' in train_metrics:
                self.train_history['gate_mean'].append(train_metrics.get('gate_mean', 0.0))
                self.train_history['gate_std'].append(train_metrics.get('gate_std', 0.0))
                self.train_history['favor_time_ratio'].append(train_metrics.get('favor_time_ratio', 0.5))
            
            # éªŒè¯
            val_metrics = self.validate(val_loader, adj_matrix)
            self.val_history['loss'].append(val_metrics['loss'])
            self.val_history['accuracy'].append(val_metrics['accuracy'])

            # è®°å½•é‡‘èæŒ‡æ ‡
            if self.compute_financial_metrics:
                for key in ['IC', 'RankIC', 'long_short_return']:
                    if key in val_metrics:
                        self.val_history[key].append(val_metrics[key])

            # æ‰“å°ç»“æœ
            print(f'Train Loss: {train_metrics["loss"]:.4f}, '
                  f'Train Acc: {train_metrics["accuracy"]:.2f}%')
            print(f'Val Loss: {val_metrics["loss"]:.4f}, '
                  f'Val Acc: {val_metrics["accuracy"]:.2f}%')

            # æ‰“å°é‡‘èæŒ‡æ ‡
            if self.compute_financial_metrics and 'IC' in val_metrics:
                print(f'Val IC: {val_metrics.get("IC", 0):.4f}, '
                      f'RankIC: {val_metrics.get("RankIC", 0):.4f}, '
                      f'Long-Short: {val_metrics.get("long_short_return", 0):.4f}')

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.use_scheduler and self.scheduler is not None:
                self.scheduler.step(val_metrics['loss'])
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f'Learning Rate: {current_lr:.6f}')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['accuracy'] > best_val_acc:
                best_val_acc = val_metrics['accuracy']
                if save_path:
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'val_accuracy': best_val_acc,
                        'val_metrics': val_metrics,
                    }, save_path)
                    print(f'Model saved to {save_path}')
        
        return {
            'train': self.train_history,
            'val': self.val_history
        }
    
    def predict(self, dataloader: DataLoader, adj_matrix: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:
        """
        é¢„æµ‹
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            adj_matrix: é‚»æ¥çŸ©é˜µ
            
        Returns:
            - é¢„æµ‹æ¦‚ç‡ï¼Œå½¢çŠ¶ä¸º [æ ·æœ¬æ•°, num_classes]
            - é¢„æµ‹ç±»åˆ«ï¼Œå½¢çŠ¶ä¸º [æ ·æœ¬æ•°]
        """
        self.model.eval()
        all_predictions = []
        
        adj_matrix = adj_matrix.to(self.device)
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc='Predicting'):
                sequences = batch['sequence'].to(self.device)
                masks = batch['mask'].to(self.device)
                industry_indices = batch['industry_idx'].to(self.device)
                
                batch_size, max_seq_len, features = sequences.shape
                
                x_80 = sequences
                x_40 = sequences[:, -40:, :]
                x_20 = sequences[:, -20:, :]
                
                mask_80 = masks
                mask_40 = masks[:, -40:]
                mask_20 = masks[:, -20:]
                
                predictions, _ = self.model(
                    x_20, x_40, x_80,
                    mask_20, mask_40, mask_80,
                    adj_matrix, industry_indices
                )
                
                all_predictions.append(predictions.cpu().numpy())
        
        all_predictions = np.concatenate(all_predictions, axis=0)
        pred_classes = np.argmax(all_predictions, axis=1)

        return all_predictions, pred_classes

    def k_fold_validate(self, dataset, adj_matrix: torch.Tensor,
                       n_splits: int = 5, min_train_size: float = 0.4,
                       num_epochs: int = 30, batch_size: int = 32,
                       save_dir: str = "./checkpoints",
                       resume_from_checkpoint: bool = True,
                       load_previous_fold: bool = False) -> Dict[str, List]:
        """
        æ—¶é—´åºåˆ—KæŠ˜äº¤å‰éªŒè¯

        Args:
            dataset: å®Œæ•´æ•°æ®é›†
            adj_matrix: é‚»æ¥çŸ©é˜µ
            n_splits: æŠ˜æ•°
            min_train_size: æœ€å°è®­ç»ƒé›†æ¯”ä¾‹
            num_epochs: æ¯æŠ˜è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            resume_from_checkpoint: æ˜¯å¦ä»checkpointæ¢å¤ï¼ˆè·³è¿‡å·²å®Œæˆçš„foldï¼‰
            load_previous_fold: æ˜¯å¦ä»ä¸Šä¸€ä¸ªfoldçš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆFalseåˆ™é‡æ–°åˆå§‹åŒ–ï¼‰

        Returns:
            KæŠ˜éªŒè¯ç»“æœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"Starting {n_splits}-Fold Time Series Cross-Validation")
        print(f"{'='*60}\n")

        # åˆ›å»ºKæŠ˜éªŒè¯å™¨
        tscv = TimeSeriesKFold(n_splits=n_splits, min_train_size=min_train_size)

        # å‡†å¤‡ç´¢å¼•æ•°ç»„
        indices = np.arange(len(dataset))

        # å­˜å‚¨æ‰€æœ‰æŠ˜çš„ç»“æœ
        fold_results = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
        }

        if self.compute_financial_metrics:
            fold_results['val_IC'] = []
            fold_results['val_RankIC'] = []
            fold_results['val_long_short'] = []

        # åˆ›å»ºä¿å­˜ç›®å½•
        Path(save_dir).mkdir(parents=True, exist_ok=True)

        # â­ æ£€æµ‹å·²å®Œæˆçš„foldï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        completed_folds = []
        if resume_from_checkpoint:
            for fold_num in range(1, n_splits + 1):
                fold_checkpoint = os.path.join(save_dir, f"fold_{fold_num}_best.pth")
                if os.path.exists(fold_checkpoint):
                    try:
                        checkpoint = torch.load(fold_checkpoint, weights_only=False, map_location='cpu')
                        # æ£€æŸ¥checkpointæ˜¯å¦å®Œæ•´ï¼ˆåŒ…å«å¿…è¦çš„é”®ï¼‰
                        if 'model_state_dict' in checkpoint and 'val_metrics' in checkpoint:
                            completed_folds.append(fold_num)
                            print(f"âœ“ Found completed fold {fold_num} checkpoint")
                    except Exception as e:
                        print(f"âš  Warning: Could not load fold {fold_num} checkpoint: {e}")
            
            if completed_folds:
                print(f"\nğŸ“‹ Resuming training: Found {len(completed_folds)} completed fold(s): {completed_folds}")
                # è®¡ç®—éœ€è¦è®­ç»ƒçš„fold
                all_folds = set(range(1, n_splits + 1))
                folds_to_train = sorted(all_folds - set(completed_folds))
                folds_to_skip = sorted(completed_folds)
                
                if folds_to_skip:
                    print(f"   âœ“ Will SKIP fold(s): {folds_to_skip} (using checkpoint results)")
                if folds_to_train:
                    print(f"   â†’ Will TRAIN fold(s): {folds_to_train}")
                else:
                    print(f"   âœ“ All folds completed! Will only load results.")
            else:
                print(f"\nğŸ“‹ No completed folds found, starting from scratch")
                print(f"   â†’ Will TRAIN all folds: {list(range(1, n_splits + 1))}")

        # KæŠ˜éªŒè¯
        for fold, (train_idx, val_idx) in enumerate(tscv.split(indices), 1):
            # â­ è·³è¿‡å·²å®Œæˆçš„fold
            if resume_from_checkpoint and fold in completed_folds:
                print(f"\n{'-'*60}")
                print(f"Fold {fold}/{n_splits} - SKIPPED (already completed)")
                print(f"{'-'*60}\n")
                
                # åŠ è½½å·²å®Œæˆçš„foldç»“æœ
                fold_checkpoint = os.path.join(save_dir, f"fold_{fold}_best.pth")
                checkpoint = torch.load(fold_checkpoint, weights_only=False, map_location='cpu')
                
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç”¨äºè¯„ä¼°
                train_subset = Subset(dataset, train_idx)
                val_subset = Subset(dataset, val_idx)
                train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, num_workers=0)
                val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0)
                
                # åŠ è½½æ¨¡å‹çŠ¶æ€
                self.model.load_state_dict(checkpoint['model_state_dict'])
                
                # è¯„ä¼°ï¼ˆå¦‚æœéœ€è¦é‡æ–°è®¡ç®—æŒ‡æ ‡ï¼‰
                final_val_metrics = self.validate(val_loader, adj_matrix)
                final_train_metrics = self.validate(train_loader, adj_matrix, compute_metrics=False)
                
                # ä½¿ç”¨checkpointä¸­çš„æŒ‡æ ‡ï¼Œæˆ–é‡æ–°è®¡ç®—çš„æŒ‡æ ‡
                if 'val_metrics' in checkpoint:
                    final_val_metrics = checkpoint['val_metrics']
                
                # è®°å½•ç»“æœ
                fold_results['train_loss'].append(final_train_metrics['loss'])
                fold_results['train_acc'].append(final_train_metrics['accuracy'])
                fold_results['val_loss'].append(final_val_metrics['loss'])
                fold_results['val_acc'].append(final_val_metrics['accuracy'])
                
                if self.compute_financial_metrics:
                    fold_results['val_IC'].append(final_val_metrics.get('IC', 0))
                    fold_results['val_RankIC'].append(final_val_metrics.get('RankIC', 0))
                    fold_results['val_long_short'].append(final_val_metrics.get('long_short_return', 0))
                
                print(f"Fold {fold} Results (from checkpoint):")
                print(f"  Val Loss: {final_val_metrics['loss']:.4f}")
                print(f"  Val Acc: {final_val_metrics['accuracy']:.2f}%")
                if self.compute_financial_metrics:
                    print(f"  Val IC: {final_val_metrics.get('IC', 0):.4f}")
                    print(f"  Val RankIC: {final_val_metrics.get('RankIC', 0):.4f}")
                
                continue
            print(f"\n{'-'*60}")
            print(f"Fold {fold}/{n_splits}")
            print(f"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}")
            print(f"{'-'*60}\n")

            # åˆ›å»ºå­æ•°æ®é›†
            train_subset = Subset(dataset, train_idx)
            val_subset = Subset(dataset, val_idx)

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = DataLoader(
                train_subset,
                batch_size=batch_size,
                shuffle=False,  # ä¿æŒæ—¶é—´é¡ºåº
                num_workers=0
            )
            val_loader = DataLoader(
                val_subset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0
            )

            # â­ æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥
            if load_previous_fold and fold > 1:
                # ä»ä¸Šä¸€ä¸ªfoldçš„checkpointåŠ è½½æ¨¡å‹
                prev_fold_checkpoint = os.path.join(save_dir, f"fold_{fold-1}_best.pth")
                if os.path.exists(prev_fold_checkpoint):
                    try:
                        prev_checkpoint = torch.load(prev_fold_checkpoint, weights_only=False, map_location=self.device)
                        self.model.load_state_dict(prev_checkpoint['model_state_dict'])
                        print(f"  âœ“ Loaded model from fold {fold-1} checkpoint")
                        
                        # å¯é€‰ï¼šä¹ŸåŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚æœcheckpointä¸­æœ‰ï¼‰
                        if 'optimizer_state_dict' in prev_checkpoint:
                            self.optimizer.load_state_dict(prev_checkpoint['optimizer_state_dict'])
                            print(f"  âœ“ Loaded optimizer from fold {fold-1} checkpoint")
                    except Exception as e:
                        print(f"  âš  Warning: Could not load fold {fold-1} checkpoint: {e}")
                        print(f"  â†’ Reinitializing model parameters")
                        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œé‡æ–°åˆå§‹åŒ–
                        for layer in self.model.children():
                            if hasattr(layer, 'reset_parameters'):
                                layer.reset_parameters()
                else:
                    # ä¸Šä¸€ä¸ªfoldçš„checkpointä¸å­˜åœ¨ï¼Œé‡æ–°åˆå§‹åŒ–
                    for layer in self.model.children():
                        if hasattr(layer, 'reset_parameters'):
                            layer.reset_parameters()
            else:
                # é‡æ–°åˆå§‹åŒ–æ¨¡å‹(æ¯æŠ˜é‡æ–°è®­ç»ƒ)
                # æ³¨æ„:è¿™é‡Œéœ€è¦ä»å¤–éƒ¨ä¼ å…¥æ¨¡å‹æ„é€ å‡½æ•°
                # ä¸ºç®€åŒ–,æˆ‘ä»¬é‡ç½®æ¨¡å‹å‚æ•°
                for layer in self.model.children():
                    if hasattr(layer, 'reset_parameters'):
                        layer.reset_parameters()

            # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆé™¤éä»checkpointåŠ è½½äº†ï¼‰
            if not (load_previous_fold and fold > 1 and os.path.exists(os.path.join(save_dir, f"fold_{fold-1}_best.pth"))):
                self.optimizer = optim.Adam(
                    self.model.parameters(),
                    lr=self.optimizer.param_groups[0]['lr'],
                    weight_decay=self.optimizer.defaults['weight_decay']
                )

            # è®­ç»ƒå½“å‰æŠ˜
            best_val_acc = 0.0
            fold_save_path = os.path.join(save_dir, f"fold_{fold}_best.pth")

            for epoch in range(num_epochs):
                print(f'Fold {fold}, Epoch {epoch + 1}/{num_epochs}')

                # è®­ç»ƒ
                train_metrics = self.train_epoch(train_loader, adj_matrix)

                # éªŒè¯
                val_metrics = self.validate(val_loader, adj_matrix)

                print(f'  Train Loss: {train_metrics["loss"]:.4f}, Acc: {train_metrics["accuracy"]:.2f}%')
                print(f'  Val Loss: {val_metrics["loss"]:.4f}, Acc: {val_metrics["accuracy"]:.2f}%')

                if self.compute_financial_metrics and 'IC' in val_metrics:
                    print(f'  Val IC: {val_metrics["IC"]:.4f}, RankIC: {val_metrics["RankIC"]:.4f}')

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_metrics['accuracy'] > best_val_acc:
                    best_val_acc = val_metrics['accuracy']
                    torch.save({
                        'fold': fold,
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),  # â­ ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
                        'val_metrics': val_metrics,
                        'best_val_acc': best_val_acc,
                    }, fold_save_path)

            # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
            checkpoint = torch.load(fold_save_path, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])

            # æœ€ç»ˆéªŒè¯
            final_val_metrics = self.validate(val_loader, adj_matrix)
            final_train_metrics = self.validate(train_loader, adj_matrix, compute_metrics=False)

            # è®°å½•ç»“æœ
            fold_results['train_loss'].append(final_train_metrics['loss'])
            fold_results['train_acc'].append(final_train_metrics['accuracy'])
            fold_results['val_loss'].append(final_val_metrics['loss'])
            fold_results['val_acc'].append(final_val_metrics['accuracy'])

            if self.compute_financial_metrics:
                fold_results['val_IC'].append(final_val_metrics.get('IC', 0))
                fold_results['val_RankIC'].append(final_val_metrics.get('RankIC', 0))
                fold_results['val_long_short'].append(final_val_metrics.get('long_short_return', 0))

            print(f"\nFold {fold} Final Results:")
            print(f"  Val Loss: {final_val_metrics['loss']:.4f}")
            print(f"  Val Acc: {final_val_metrics['accuracy']:.2f}%")
            if self.compute_financial_metrics:
                print(f"  Val IC: {final_val_metrics.get('IC', 0):.4f}")
                print(f"  Val RankIC: {final_val_metrics.get('RankIC', 0):.4f}")

        # æ‰“å°æ±‡æ€»ç»“æœ
        print(f"\n{'='*60}")
        print(f"K-Fold Cross-Validation Summary")
        print(f"{'='*60}\n")

        print(f"Average Train Loss: {np.mean(fold_results['train_loss']):.4f} Â± {np.std(fold_results['train_loss']):.4f}")
        print(f"Average Train Acc: {np.mean(fold_results['train_acc']):.2f}% Â± {np.std(fold_results['train_acc']):.2f}%")
        print(f"Average Val Loss: {np.mean(fold_results['val_loss']):.4f} Â± {np.std(fold_results['val_loss']):.4f}")
        print(f"Average Val Acc: {np.mean(fold_results['val_acc']):.2f}% Â± {np.std(fold_results['val_acc']):.2f}%")

        if self.compute_financial_metrics:
            print(f"\nFinancial Metrics:")
            print(f"Average IC: {np.mean(fold_results['val_IC']):.4f} Â± {np.std(fold_results['val_IC']):.4f}")
            print(f"Average RankIC: {np.mean(fold_results['val_RankIC']):.4f} Â± {np.std(fold_results['val_RankIC']):.4f}")
            print(f"Average Long-Short: {np.mean(fold_results['val_long_short']):.4f} Â± {np.std(fold_results['val_long_short']):.4f}")

        # â­ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆé€‰æ‹©éªŒè¯å‡†ç¡®ç‡æœ€é«˜çš„foldï¼‰
        best_fold_idx = np.argmax(fold_results['val_acc'])
        best_fold = best_fold_idx + 1  # foldç¼–å·ä»1å¼€å§‹
        best_fold_path = os.path.join(save_dir, f"fold_{best_fold}_best.pth")
        best_model_path = os.path.join(save_dir, "best_model.pth")
        
        # å¤åˆ¶æœ€ä½³foldçš„æ¨¡å‹ä¸ºbest_model.pth
        if os.path.exists(best_fold_path):
            shutil.copy2(best_fold_path, best_model_path)
            print(f"\nâœ“ Best model saved: {best_model_path} (from Fold {best_fold}, Val Acc: {fold_results['val_acc'][best_fold_idx]:.2f}%)")
        else:
            print(f"\nâš  Warning: Could not find {best_fold_path} to save as best_model.pth")

        return fold_results

